namespace Mostlylucid.DocSummarizer.Images.Models.Dynamic;

/// <summary>
/// Human-readable descriptions for signals, used in detailed reports and documentation.
/// Provides explanatory text for each signal key to make analysis results more understandable.
/// </summary>
public static class SignalDescriptions
{
    private static readonly Dictionary<string, SignalDescription> Descriptions = new(StringComparer.OrdinalIgnoreCase)
    {
        // Identity signals
        ["identity.format"] = new("Image Format", "The file format (GIF, PNG, JPEG, etc.) detected from the image header."),
        ["identity.width"] = new("Width", "Image width in pixels."),
        ["identity.height"] = new("Height", "Image height in pixels."),
        ["identity.aspect_ratio"] = new("Aspect Ratio", "Width divided by height (e.g., 1.78 for 16:9)."),
        ["identity.file_size"] = new("File Size", "Size of the image file in bytes."),
        ["identity.is_animated"] = new("Animated", "Whether the image contains multiple frames (animated GIF, APNG)."),
        ["identity.frame_count"] = new("Frame Count", "Number of frames in an animated image."),

        // Color signals
        ["color.dominant_colors"] = new("Dominant Colors", "The most prevalent colors in the image, with percentage coverage."),
        ["color.unique_count"] = new("Unique Colors", "Approximate number of distinct colors in the image."),
        ["color.mean_saturation"] = new("Mean Saturation", "Average color saturation (0 = grayscale, 1 = fully saturated)."),
        ["color.is_grayscale"] = new("Grayscale", "Whether the image is primarily black and white or sepia."),
        ["color.palette"] = new("Color Palette", "Named colors representing the image's color scheme."),
        ["color.vibrant"] = new("Vibrant Color", "The most vivid, eye-catching color in the image."),
        ["color.muted"] = new("Muted Color", "The most subdued, soft color in the image."),

        // Motion signals
        ["motion.has_motion"] = new("Has Motion", "Whether significant motion was detected between frames."),
        ["motion.type"] = new("Motion Type", "Classification: camera_pan, camera_zoom, object_motion, or general."),
        ["motion.direction"] = new("Motion Direction", "Primary direction of movement: left, right, up, down, stationary."),
        ["motion.magnitude"] = new("Motion Magnitude", "Intensity of detected motion (higher = more movement)."),
        ["motion.activity"] = new("Motion Activity", "Percentage of the image area that contains motion."),
        ["motion.summary"] = new("Motion Summary", "Human-readable description of the motion pattern."),
        ["motion.moving_objects"] = new("Moving Objects", "List of identified objects that are in motion."),
        ["motion.temporal_consistency"] = new("Temporal Consistency", "How consistent the motion is across frames (1 = very consistent)."),
        ["motion.is_looping"] = new("Looping", "Whether the animation loops seamlessly."),
        ["motion.regions"] = new("Motion Regions", "Areas of the image where motion was detected."),

        // OCR signals
        ["ocr.full_text"] = new("OCR Text", "All text extracted from the image using optical character recognition."),
        ["ocr.text_region"] = new("Text Region", "A specific area containing detected text with bounding box."),
        ["ocr.confidence"] = new("OCR Confidence", "How confident the OCR engine is in the extracted text (0-1)."),
        ["ocr.no_text_found"] = new("No Text Found", "OCR was performed but no readable text was detected."),
        ["ocr.unavailable"] = new("OCR Unavailable", "Tesseract data not found - use Vision LLM mode instead."),
        ["ocr.skipped"] = new("OCR Skipped", "OCR was skipped because text likelihood was too low."),
        ["ocr.voting.consensus_text"] = new("Voting Consensus", "Text agreed upon by multiple frame OCR passes."),
        ["ocr.quality.spell_check_score"] = new("Spell Check Score", "Percentage of recognized words that are valid dictionary words."),
        ["ocr.quality.is_garbled"] = new("Garbled Text", "Whether the OCR output appears to be noise rather than real text."),

        // Vision LLM signals
        ["vision.llm.caption"] = new("Caption", "A concise description of the image generated by Vision LLM."),
        ["vision.llm.detailed_description"] = new("Detailed Description", "Comprehensive analysis of the image content, setting, and mood."),
        ["vision.llm.scene"] = new("Scene Type", "Classification of the scene: indoor, outdoor, food, nature, urban, document, screenshot, meme, art."),
        ["vision.llm.entities"] = new("Detected Entities", "People, animals, objects, and text identified in the image."),
        ["vision.llm.text"] = new("LLM Text Reading", "Text extracted by Vision LLM - often better for stylized/artistic fonts."),
        ["vision.llm.disabled"] = new("Vision LLM Disabled", "Vision LLM analysis was not enabled for this run."),
        ["vision.llm.error"] = new("Vision LLM Error", "An error occurred during Vision LLM analysis."),

        // Quality signals
        ["quality.sharpness"] = new("Sharpness", "Image sharpness score - higher values indicate crisper details."),
        ["quality.blur"] = new("Blur", "Amount of blur detected (0 = sharp, 1 = very blurry)."),
        ["quality.noise"] = new("Noise", "Level of digital noise or grain in the image."),
        ["quality.compression_artifacts"] = new("Compression Artifacts", "Visible JPEG compression blocks or GIF banding."),
        ["quality.overall"] = new("Overall Quality", "Combined quality assessment score."),

        // Content signals
        ["content.text_likeliness"] = new("Text Likelihood", "Probability that the image contains readable text (0-1)."),
        ["content.salient_regions"] = new("Salient Regions", "Areas of the image that draw visual attention."),

        // Visual/Composition signals
        ["visual.edge_density"] = new("Edge Density", "Amount of edges/detail in the image (higher = more complex)."),
        ["visual.complexity"] = new("Visual Complexity", "Overall visual complexity score."),
        ["visual.mean_luminance"] = new("Mean Brightness", "Average brightness of the image (0 = black, 1 = white)."),
        ["composition.symmetry"] = new("Symmetry", "How symmetrical the image composition is."),
        ["composition.rule_of_thirds"] = new("Rule of Thirds", "How well the composition follows the rule of thirds.")
    };

    /// <summary>
    /// Get the description for a signal key
    /// </summary>
    public static SignalDescription? GetDescription(string signalKey)
    {
        // Try exact match first
        if (Descriptions.TryGetValue(signalKey, out var desc))
            return desc;

        // Try prefix match for entity types like "vision.llm.entity.person"
        var prefix = signalKey.Contains('.')
            ? string.Join(".", signalKey.Split('.').Take(3))
            : signalKey;

        if (Descriptions.TryGetValue(prefix, out desc))
            return desc;

        return null;
    }

    /// <summary>
    /// Get all registered signal descriptions
    /// </summary>
    public static IReadOnlyDictionary<string, SignalDescription> All => Descriptions;

    /// <summary>
    /// Format a signal for display with its description
    /// </summary>
    public static string FormatWithDescription(string key, object? value, double confidence)
    {
        var desc = GetDescription(key);
        var name = desc?.Name ?? key;
        var explanation = desc?.Description ?? "";

        var valueStr = value?.ToString() ?? "null";
        if (valueStr.Length > 100)
            valueStr = valueStr.Substring(0, 100) + "...";

        return $"**{name}** ({confidence:P0}): {valueStr}\n  _{explanation}_";
    }
}

/// <summary>
/// Description for a signal type
/// </summary>
public record SignalDescription(string Name, string Description);
